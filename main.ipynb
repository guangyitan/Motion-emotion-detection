{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libaries \n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "# from google.colab.patches import cv2_imshow\n",
    "\n",
    "# read the video using VideoCapture funtion\n",
    "cap = cv2.VideoCapture(os.getcwd() + \"/data/clip2_7s.mp4\")\n",
    "\n",
    "try:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        cv2.imshow(\"inter\", frame) \n",
    "        #   cv2_imshow(frame) # for google colab only\n",
    "\n",
    "        # esc key = 27\n",
    "        if cv2.waitKey(40) == 27:\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release\n",
    "\n",
    "except:\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\User\\\\Coding_Projects\\\\Python Projects\\\\Motion-emotion-detection'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sucessful\n"
     ]
    }
   ],
   "source": [
    "# Setting GPU-enabled Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      # tf.config.experimental.set_virtual_device_configuration(gpu, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "      # tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "      print(\"sucessful\")\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable gpu\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n",
      "{'emotion': {'angry': 27.076569199562073, 'disgust': 0.00011799746744145523, 'fear': 59.964632987976074, 'happy': 12.385643273591995, 'sad': 0.5619456525892019, 'surprise': 0.0007821342478564475, 'neutral': 0.010309615026926622}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 87, 'h': 92}}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "# # face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "# print(len(tf.config.list_physical_devices('GPU'))>0)\n",
    "\n",
    "def analyse_face():\n",
    "    # imagepath = os.getcwd() + \"/data/happy_face_woman.png\"\n",
    "    imagepath = os.getcwd() + \"/data/face.png\"\n",
    "    # imagepath = os.getcwd() + \"/data/class.png\"\n",
    "    image = cv2.imread(imagepath)\n",
    "    # face_analysis = DeepFace.analyze(image)\n",
    "    face_analysis = DeepFace.analyze(image, actions=['emotion'], enforce_detection=False)\n",
    "    print(face_analysis)\n",
    "\n",
    "analyse_face()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (image) face + emotion detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "filename = \"neutral.png\"\n",
    "# filename = \"happy.png\"\n",
    "# filename = \"sad.png\"\n",
    "\n",
    "imagepath = os.getcwd() + \"/data/\" + filename\n",
    "image = cv2.imread(imagepath)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor = 1.1, minNeighbors = 5)\n",
    "for (x,y,w,h) in faces:\n",
    "    temp = (x,y,w,h)\n",
    "    image = cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    color = (255,0,0)\n",
    "    try:\n",
    "        analyze = DeepFace.analyze(image[y:y+h, x:x+w], actions=['emotion'], enforce_detection=False)\n",
    "        emotion_label = analyze['dominant_emotion']\n",
    "        state_of_mind_label = get_state_of_mind(analyze['emotion'])\n",
    "        image=cv2.putText(image, emotion_label, (x, y - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "        image=cv2.putText(image, state_of_mind_label, (x, y + h + 12 ),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "cv2.imshow(\"image\", image) \n",
    "cv2.imwrite(os.getcwd() + \"/output/\" + filename , image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'angry': 4.505497485726922,\n",
       " 'disgust': 0.00010644097935775428,\n",
       " 'fear': 85.11115819878329,\n",
       " 'happy': 8.703607839560975,\n",
       " 'sad': 1.666227082385247,\n",
       " 'surprise': 0.0012948668744324054,\n",
       " 'neutral': 0.012110988603768191}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze['emotion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State of mind Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in a dictionary of confidence score for each emotion\n",
    "def get_state_of_mind(emotions_dict):\n",
    "    # declaring variables    \n",
    "    confusion_list = ['neutral', 'surprise'] # duplicated combinatorial emotions for 'sad' with 'Disappointment/dissatisfaction', so 'sad' is removed from 'Confusion'\n",
    "    satisfaction_delighterd_list = ['happy', 'neutral']\n",
    "    disappointment_disstisfaction_list = ['neutral', 'sad']\n",
    "    frustrated_list = ['sad', 'angry', 'neutral']\n",
    "    # dictionary of ecombinatorial emotions with its respective state of mind\n",
    "    state_of_mind_dict = {'Confusion': confusion_list, \n",
    "    'Satisfaction/Delighted': satisfaction_delighterd_list, \n",
    "    'Disappointment/Dissatisfaction': disappointment_disstisfaction_list,\n",
    "    'Frustrated': frustrated_list}\n",
    "\n",
    "    # dictionary used to store and determin the state of mind based on the combinatorial emotions\n",
    "    state_of_mind_count = {'Confusion': [0, 0], \n",
    "    'Satisfaction/Delighted': [0, 0], \n",
    "    'Disappointment/Dissatisfaction': [0, 0],\n",
    "    'Frustrated': [0, 0, 0]}\n",
    "    state_of_mind_list = ['Confusion', 'Satisfaction/Delighted', 'Disappointment/Dissatisfaction', 'Frustrated']\n",
    "    count = 0\n",
    "    state_switched = False\n",
    "\n",
    "    # sort emotion from highest confidence score to lowest\n",
    "    # sorted_emotions = emotions_dict # TODO: use for testing\n",
    "    sorted_emotions = sorted(emotions_dict, key=emotions_dict.get, reverse=True)[:]\n",
    "    for emotion in sorted_emotions:\n",
    "        for state in state_of_mind_list:\n",
    "            # if the emotion belongs to a certain state of mind\n",
    "            if emotion in state_of_mind_dict[state]: \n",
    "                if count == 0:\n",
    "                    state_of_mind_list.remove(state)\n",
    "                    state_of_mind_list.insert(0, state)\n",
    "                    state_switched = True\n",
    "                # get the index of the emotion in the dictionary\n",
    "                idx = state_of_mind_dict[state].index(emotion)\n",
    "                # set the count to 1\n",
    "                state_of_mind_count[state][idx] = 1\n",
    "            \n",
    "            # check if all emotions of a certain state of mind were matched\n",
    "            if sum(state_of_mind_count[state]) == len(state_of_mind_count[state]):\n",
    "                return state\n",
    "        \n",
    "        if state_switched:\n",
    "            count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Confusion': [0, 0], 'Satisfaction/Delighted': [1, 0], 'Disappointment/Dissatisfaction': [0, 1], 'Frustrated': [1, 1, 1]}\n",
      "Frustrated\n",
      "matched state of mind!\n"
     ]
    }
   ],
   "source": [
    "# test case for state of mind algorithm\n",
    "testcase1 = ['fear', 'neutral', 'angry', 'surprise', 'happy', 'sad', 'disgust'] #neutral + suprise = confusion\n",
    "testcase2 = ['fear', 'happy', 'angry', 'sad', 'neutral', 'surprise', 'disgust'] #Happy + Neutral = Satisfaction / Delighted\n",
    "testcase3 = ['fear', 'sad', 'neutral', 'disgust', 'angry', 'surprise', 'happy'] #Neutral + Sadness = Disappointment / Dissatisfaction\n",
    "testcase4 = ['fear', 'sad', 'angry', 'happy', 'neutral', 'surprise', 'disgust'] #Sad + Angry + Neutral = Frustrated\n",
    "testcase5 = ['fear', 'happy', 'angry', 'sad', 'neutral', 'surprise', 'disgust']\n",
    "\n",
    "testcase_all = [testcase1, testcase2, testcase3, testcase4, testcase5]\n",
    "\n",
    "# get_state_of_mind(analyze['emotion'])\n",
    "\n",
    "# for x in range(len(testcase_all)):\n",
    "#     print(\"case \", x+1)\n",
    "#     get_state_of_mind(testcase_all[x])\n",
    "\n",
    "s = get_state_of_mind(testcase4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Confusion': [1, 1],\n",
       " 'Satisfaction/Delighted': [1, 1],\n",
       " 'Disappointment/Dissatisfaction': [1, 0, 0],\n",
       " 'Frustrated': [1, 1, 1]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_of_mind_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_of_mind_count['Disappointment/Dissatisfaction'][1] = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_emotions = ['fear', 'happy', 'angry', 'sad', 'neutral', 'surprise', 'disgust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Confusion': [0, 0],\n",
       " 'Satisfaction/Delighted': [0, 0],\n",
       " 'Disappointment/Dissatisfaction': [0, 0, 0],\n",
       " 'Frustrated': [0, 0, 0]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_of_mind_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# video face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "filename = 'video_1'\n",
    "# filename = 'video_2'\n",
    "\n",
    "# declare the facce detection model to detect human faces\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# read the video using VideoCapture funtion\n",
    "# video = cv2.VideoCapture(0)\n",
    "video = cv2.VideoCapture(os.getcwd() + \"/data/\" + filename + \".mp4\")\n",
    "\n",
    "# get width and height\n",
    "frame_width = int(video.get(3))\n",
    "frame_height = int(video.get(4))\n",
    "\n",
    "# declare a VideoWriter instance\n",
    "result_mp4 = cv2.VideoWriter(os.getcwd() + \"/output/\" + filename + '_result.mp4', cv2.VideoWriter_fourcc(*'MP4V'), 10, (frame_width, frame_height))\n",
    "\n",
    "try:\n",
    "    while video.isOpened():\n",
    "        # read the video into frame\n",
    "        ret, frame = video.read()\n",
    "        ret, frame1 = video.read() \n",
    "\n",
    "        # A. face detection, emotion classification, state of mind\n",
    "        # A1. convert the frame to grayscale for CascadeClassifier\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # A2. find coordinates of faces, returns (x,y,w,h)\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor = 1.1, minNeighbors = 5)\n",
    "\n",
    "        # B. motion detection\n",
    "        # B1. find the absolute difference between current frame and previous frame\n",
    "        diff = cv2.absdiff(frame, frame1)\n",
    "        # B2. convert the frame difference to grayscale \n",
    "        gray = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "        # B3. apply gaussian smoothing onto the grayscale frame to reduce noise \n",
    "        blur = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "        # B4. apply thresholding \n",
    "        _, thresh = cv2.threshold(blur, 20, 255, cv2.THRESH_BINARY)\n",
    "        # B5. dilate the frame to remove small unwanted detection\n",
    "        dilated = cv2.dilate(thresh, None, iterations=3)\n",
    "        # B6. get the contours of detected movement in frame\n",
    "        contours, _ = cv2.findContours(dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # legends in video\n",
    "        cv2.putText(frame, \"Blue: Emotion\", (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "        cv2.putText(frame, \"Red: State of Mind\", (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "        cv2.putText(frame, \"Green: Motion\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # loop through all detected faces\n",
    "        for (x,y,w,h) in faces:\n",
    "            color = (255,0,0) # blue color\n",
    "            # A3. draw a blue rectangle around the face\n",
    "            frame = cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            try:\n",
    "                # A4. perform emotion classification on the face detected\n",
    "                analyze = DeepFace.analyze(frame[y:y+h, x:x+w], actions=['emotion'], enforce_detection=False)\n",
    "                # A5. get the dominant emotion as the label \n",
    "                emotion_label = analyze['dominant_emotion']\n",
    "                # A6. perform state of mind calculation\n",
    "                state_of_mind_label = get_state_of_mind(analyze['emotion'])\n",
    "                # A7. write the emotion on the frame in blue text\n",
    "                frame=cv2.putText(frame, emotion_label, (x, y - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "                # A8. write the state of mind on the frame in red text\n",
    "                frame=cv2.putText(frame, state_of_mind_label, (x, y + h + 12 ),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        # loop through all contours\n",
    "        for contour in contours:\n",
    "            # B7. generate x, y coordinate, width, height based on the contour\n",
    "            (x, y, w, h) = cv2.boundingRect(contour)\n",
    "            # B8. draw a green rectangle if the contour area is greater than 600\n",
    "            if cv2.contourArea(contour) > 600:\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        result_mp4.write(frame)\n",
    "\n",
    "        cv2.imshow(\"video\", frame) \n",
    "        frame = frame1\n",
    "        ret, frame1 = video.read()\n",
    "        #   cv2_imshow(frame) # for google colab only\n",
    "        # cv2.waitKey(5000)\n",
    "        if cv2.waitKey(40) == 27:\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release\n",
    "    result_mp4.release()\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "cv2.destroyAllWindows()\n",
    "video.release\n",
    "result_mp4.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# read the video and extract info about it\n",
    "# cap = cv2.VideoCapture('vid.avi')\n",
    "cap = cv2.VideoCapture(os.getcwd() + \"/data/clip2_7s.mp4\")\n",
    "\n",
    "\n",
    "# get total number of frames and generate a list with each 30 th frame \n",
    "totalFrames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "x = [i for i in range (1, totalFrames) if divmod(i, int(30))[1]==0]\n",
    "\n",
    "\n",
    "for myFrameNumber in x:\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES,myFrameNumber)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        cv2.imshow(\"video\", frame)\n",
    "\n",
    "        ch = 0xFF & cv2.waitKey(1000)\n",
    "        if ch == 27:\n",
    "            break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 60, 90, 120, 150]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.9.2\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: c:\\users\\guang\\anaconda3\\envs\\deepfacepy37\\lib\\site-packages\n",
      "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, keras-preprocessing, libclang, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
      "Required-by: deepface, retina-face\n"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 14 22:51:31 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 451.83       Driver Version: 451.83       CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce MX450      WDDM  | 00000000:5A:00.0 Off |                  N/A |\n",
      "| N/A   47C    P8    N/A /  N/A |    119MiB /  2048MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 10 01:44:05 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 451.83       Driver Version: 451.83       CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce MX450      WDDM  | 00000000:5A:00.0 Off |                  N/A |\n",
      "| N/A   48C    P8    N/A /  N/A |   1432MiB /  2048MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     14196      C   ...s\\deepFacepy37\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('deepFacepy37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21b6fde438783047a9b28b66d2ac1a8262ef604e581950bf28e988f9a8bd21db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
