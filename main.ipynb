{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libaries \n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "# from google.colab.patches import cv2_imshow\n",
    "\n",
    "# read the video using VideoCapture funtion\n",
    "cap = cv2.VideoCapture(os.getcwd() + \"/data/clip2_7s.mp4\")\n",
    "\n",
    "try:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        cv2.imshow(\"inter\", frame) \n",
    "        #   cv2_imshow(frame) # for google colab only\n",
    "\n",
    "        # esc key = 27\n",
    "        if cv2.waitKey(40) == 27:\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release\n",
    "\n",
    "except:\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\User\\\\Coding_Projects\\\\Python Projects\\\\Motion-emotion-detection'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sucessful\n"
     ]
    }
   ],
   "source": [
    "# Setting GPU-enabled Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      # tf.config.experimental.set_virtual_device_configuration(gpu, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "      # tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "      print(\"sucessful\")\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable gpu\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "{'emotion': {'angry': 27.07657814025879, 'disgust': 0.00011799826324931928, 'fear': 59.964632987976074, 'happy': 12.385634332895279, 'sad': 0.5619451403617859, 'surprise': 0.0007821348845027387, 'neutral': 0.010309625940863043}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 87, 'h': 92}}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "# # face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "# print(len(tf.config.list_physical_devices('GPU'))>0)\n",
    "\n",
    "def analyse_face():\n",
    "    # imagepath = os.getcwd() + \"/data/happy_face_woman.png\"\n",
    "    imagepath = os.getcwd() + \"/data/face.png\"\n",
    "    # imagepath = os.getcwd() + \"/data/class.png\"\n",
    "    image = cv2.imread(imagepath)\n",
    "    # face_analysis = DeepFace.analyze(image)\n",
    "    face_analysis = DeepFace.analyze(image, actions=['emotion'], enforce_detection=False)\n",
    "    print(face_analysis)\n",
    "\n",
    "analyse_face()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image face + emotion detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1190, 201, 58, 58)\n",
      "1190 201 58 58\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "{'emotion': {'angry': 1.3205617377831624, 'disgust': 0.00010861865733828805, 'fear': 0.6596325751701086, 'happy': 0.109685586336772, 'sad': 3.4279894444929893, 'surprise': 0.008387126853738202, 'neutral': 94.47363549156611}, 'dominant_emotion': 'neutral', 'region': {'x': 2, 'y': 3, 'w': 52, 'h': 52}}\n",
      "(258, 207, 63, 63)\n",
      "258 207 63 63\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "{'emotion': {'angry': 17.88131296634674, 'disgust': 0.023100583348423243, 'fear': 10.337983816862106, 'happy': 0.13597971992567182, 'sad': 57.24155306816101, 'surprise': 0.004338574217399582, 'neutral': 14.375734329223633}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 63, 'h': 63}}\n",
      "(700, 200, 74, 74)\n",
      "700 200 74 74\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "{'emotion': {'angry': 4.5055090341275355, 'disgust': 0.00010644137726171002, 'fear': 85.11114031738879, 'happy': 8.70361603520012, 'sad': 1.666227082385247, 'surprise': 0.0012948677839271614, 'neutral': 0.012110975507043705}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 74, 'h': 74}}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# read the video using VideoCapture funtion\n",
    "imagepath = os.getcwd() + \"/data/class.png\"\n",
    "image = cv2.imread(imagepath)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor = 1.1, minNeighbors = 5)\n",
    "for (x,y,w,h) in faces:\n",
    "    temp = (x,y,w,h)\n",
    "    print(temp)\n",
    "    print(x, y, w, h)\n",
    "    image = cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    # print(frame)\n",
    "    color = (255,0,0)\n",
    "    # label = \"emotion\"\n",
    "    # image=cv2.putText(image, label, (x, y - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "    # label = \"{}: {:.2f}%\".format(label, max(incorrectMask, mask, withoutMask) * 100)\n",
    "    # frame=cv2.putText(frame, label, (startX, startY - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "    try:\n",
    "        analyze = DeepFace.analyze(image[y:y+h, x:x+w], actions=['emotion'], enforce_detection=False)\n",
    "        # if I set the enforce_detection as False, then though MTCNN is not detecting any face in the picture, the library will consider the whole input image as a face and compute its embedding\n",
    "        # analyze = DeepFace.analyze(image, actions=['emotion'], enforce_detection=False)\n",
    "        print(analyze)\n",
    "        label = analyze['dominant_emotion']\n",
    "        image=cv2.putText(image, label, (x, y - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    # frame[y1:y2, x1:x2]\n",
    "\n",
    "# cv2.imshow(\"image\", image[temp[1]:temp[1]+temp[3], temp[0]:temp[0]+temp[2]]) \n",
    "cv2.imshow(\"image\", image) \n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# video face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# read the video using VideoCapture funtion\n",
    "video = cv2.VideoCapture(0)\n",
    "# video = cv2.VideoCapture(os.getcwd() + \"/data/clip2_7s.mp4\")\n",
    "# video = cv2.VideoCapture(os.getcwd() + \"/data/vid2_part1.mp4\")\n",
    "# video = cv2.VideoCapture(os.getcwd() + \"/data/vid1_short.mp4\")\n",
    "\n",
    "try:\n",
    "    while video.isOpened():\n",
    "    # read the video into farme\n",
    "        ret, frame = video.read()\n",
    "\n",
    "        # convert the frame to grey for CascadeClassifier\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # find coordinates of faces, returns (x,y,w,h)\n",
    "        # faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor = 1.1, minNeighbors = 5)\n",
    "\n",
    "        for (x,y,w,h) in faces:\n",
    "            frame = cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "            # print(frame)\n",
    "            color = (255,0,0)\n",
    "            # label = \"emotion\"\n",
    "            # frame=cv2.putText(frame, label, (x, y - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "            # label = \"{}: {:.2f}%\".format(label, max(incorrectMask, mask, withoutMask) * 100)\n",
    "            # frame=cv2.putText(frame, label, (startX, startY - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "            try:\n",
    "                analyze = DeepFace.analyze(frame[y:y+h, x:x+w], actions=['emotion'], enforce_detection=False)\n",
    "                # if I set the enforce_detection as False, then though MTCNN is not detecting any face in the picture, the library will consider the whole input image as a face and compute its embedding\n",
    "                # analyze = DeepFace.analyze(image, actions=['emotion'], enforce_detection=False)\n",
    "                print(analyze)\n",
    "                label = analyze['dominant_emotion']\n",
    "                frame=cv2.putText(frame, label, (x, y - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        # frame[y1:y2, x1:x2]\n",
    "\n",
    "        cv2.imshow(\"video\", frame) \n",
    "        #   cv2_imshow(frame) # for google colab only\n",
    "        if cv2.waitKey(40) == 27:\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "cv2.destroyAllWindows()\n",
    "video.release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.9.2\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: c:\\users\\guang\\anaconda3\\envs\\deepfacepy37\\lib\\site-packages\n",
      "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, keras-preprocessing, libclang, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
      "Required-by: deepface, retina-face\n"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 14 22:51:31 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 451.83       Driver Version: 451.83       CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce MX450      WDDM  | 00000000:5A:00.0 Off |                  N/A |\n",
      "| N/A   47C    P8    N/A /  N/A |    119MiB /  2048MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 10 01:44:05 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 451.83       Driver Version: 451.83       CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce MX450      WDDM  | 00000000:5A:00.0 Off |                  N/A |\n",
      "| N/A   48C    P8    N/A /  N/A |   1432MiB /  2048MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     14196      C   ...s\\deepFacepy37\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('deepFacepy37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21b6fde438783047a9b28b66d2ac1a8262ef604e581950bf28e988f9a8bd21db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
