{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libaries \n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "# from google.colab.patches import cv2_imshow\n",
    "\n",
    "# read the video using VideoCapture funtion\n",
    "cap = cv2.VideoCapture(os.getcwd() + \"/data/clip2_7s.mp4\")\n",
    "\n",
    "try:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        cv2.imshow(\"inter\", frame) \n",
    "        #   cv2_imshow(frame) # for google colab only\n",
    "\n",
    "        # esc key = 27\n",
    "        if cv2.waitKey(40) == 27:\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release\n",
    "\n",
    "except:\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\User\\\\Coding_Projects\\\\Python Projects\\\\Motion-emotion-detection'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sucessful\n"
     ]
    }
   ],
   "source": [
    "# Setting GPU-enabled Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      # tf.config.experimental.set_virtual_device_configuration(gpu, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "      # tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "      print(\"sucessful\")\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable gpu\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n",
      "{'emotion': {'angry': 27.076569199562073, 'disgust': 0.00011799746744145523, 'fear': 59.964632987976074, 'happy': 12.385643273591995, 'sad': 0.5619456525892019, 'surprise': 0.0007821342478564475, 'neutral': 0.010309615026926622}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 87, 'h': 92}}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "# # face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "# print(len(tf.config.list_physical_devices('GPU'))>0)\n",
    "\n",
    "def analyse_face():\n",
    "    # imagepath = os.getcwd() + \"/data/happy_face_woman.png\"\n",
    "    imagepath = os.getcwd() + \"/data/face.png\"\n",
    "    # imagepath = os.getcwd() + \"/data/class.png\"\n",
    "    image = cv2.imread(imagepath)\n",
    "    # face_analysis = DeepFace.analyze(image)\n",
    "    face_analysis = DeepFace.analyze(image, actions=['emotion'], enforce_detection=False)\n",
    "    print(face_analysis)\n",
    "\n",
    "analyse_face()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image face + emotion detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1190, 201, 58, 58)\n",
      "1190 201 58 58\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "{'emotion': {'angry': 1.3205610858572825, 'disgust': 0.00010861906661095263, 'fear': 0.6596329011330486, 'happy': 0.10968569111057414, 'sad': 3.4279894444929893, 'surprise': 0.008387134857292531, 'neutral': 94.47363549156611}, 'dominant_emotion': 'neutral', 'region': {'x': 2, 'y': 3, 'w': 52, 'h': 52}}\n",
      "(258, 207, 63, 63)\n",
      "258 207 63 63\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "{'emotion': {'angry': 17.881336383899235, 'disgust': 0.02310058763570861, 'fear': 10.337976982472956, 'happy': 0.13597975131376094, 'sad': 57.24153263816432, 'surprise': 0.004338572657009261, 'neutral': 14.37573667620043}, 'dominant_emotion': 'sad', 'region': {'x': 0, 'y': 0, 'w': 63, 'h': 63}}\n",
      "(700, 200, 74, 74)\n",
      "700 200 74 74\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "{'emotion': {'angry': 4.505497485726922, 'disgust': 0.00010644097935775428, 'fear': 85.11115819878329, 'happy': 8.703607839560975, 'sad': 1.666227082385247, 'surprise': 0.0012948668744324054, 'neutral': 0.012110988603768191}, 'dominant_emotion': 'fear', 'region': {'x': 0, 'y': 0, 'w': 74, 'h': 74}}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# read the video using VideoCapture funtion\n",
    "imagepath = os.getcwd() + \"/data/class.png\"\n",
    "image = cv2.imread(imagepath)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor = 1.1, minNeighbors = 5)\n",
    "for (x,y,w,h) in faces:\n",
    "    temp = (x,y,w,h)\n",
    "    print(temp)\n",
    "    print(x, y, w, h)\n",
    "    image = cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    # print(frame)\n",
    "    color = (255,0,0)\n",
    "    # label = \"emotion\"\n",
    "    # image=cv2.putText(image, label, (x, y - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "    # label = \"{}: {:.2f}%\".format(label, max(incorrectMask, mask, withoutMask) * 100)\n",
    "    # frame=cv2.putText(frame, label, (startX, startY - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "    try:\n",
    "        analyze = DeepFace.analyze(image[y:y+h, x:x+w], actions=['emotion'], enforce_detection=False)\n",
    "        # if I set the enforce_detection as False, then though MTCNN is not detecting any face in the picture, the library will consider the whole input image as a face and compute its embedding\n",
    "        # analyze = DeepFace.analyze(image, actions=['emotion'], enforce_detection=False)\n",
    "        print(analyze)\n",
    "        label = analyze['dominant_emotion']\n",
    "        image=cv2.putText(image, label, (x, y - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    # frame[y1:y2, x1:x2]\n",
    "\n",
    "# cv2.imshow(\"image\", image[temp[1]:temp[1]+temp[3], temp[0]:temp[0]+temp[2]]) \n",
    "cv2.imshow(\"image\", image) \n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'angry': 4.505497485726922,\n",
       " 'disgust': 0.00010644097935775428,\n",
       " 'fear': 85.11115819878329,\n",
       " 'happy': 8.703607839560975,\n",
       " 'sad': 1.666227082385247,\n",
       " 'surprise': 0.0012948668744324054,\n",
       " 'neutral': 0.012110988603768191}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fear'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(analyze['emotion'], key=analyze['emotion'].get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fear', 'happy', 'angry', 'sad', 'neutral', 'surprise', 'disgust']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "my_keys = sorted(analyze['emotion'], key=analyze['emotion'].get, reverse=True)[:]\n",
    "print(my_keys)\n",
    "print(type(my_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Confusion', 'Satisfaction/Delighted', 'Disappointment/Dissatisfaction', 'Frustrated']\n",
      "['Satisfaction/Delighted', 'Confusion', 'Disappointment/Dissatisfaction', 'Frustrated']\n"
     ]
    }
   ],
   "source": [
    "state_of_mind_list = ['Confusion', 'Satisfaction/Delighted', 'Disappointment/Dissatisfaction', 'Frustrated']\n",
    "print(state_of_mind_list)\n",
    "state_of_mind_list.remove('Satisfaction/Delighted')\n",
    "state_of_mind_list.insert(0, 'Satisfaction/Delighted')\n",
    "print(state_of_mind_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion:  fear\n",
      "state:  Confusion\n",
      "state:  Satisfaction/Delighted\n",
      "state:  Disappointment/Dissatisfaction\n",
      "state:  Frustrated\n",
      "emotion:  happy\n",
      "state:  Confusion\n",
      "state:  Satisfaction/Delighted\n",
      "idx 0\n",
      "{'Confusion': [0, 0], 'Satisfaction/Delighted': [1, 0], 'Disappointment/Dissatisfaction': [0, 0, 0], 'Frustrated': [0, 0, 0]}\n",
      "state:  Disappointment/Dissatisfaction\n",
      "state:  Frustrated\n",
      "emotion:  angry\n",
      "state:  Satisfaction/Delighted\n",
      "state:  Confusion\n",
      "state:  Disappointment/Dissatisfaction\n",
      "state:  Frustrated\n",
      "idx 1\n",
      "{'Confusion': [0, 0], 'Satisfaction/Delighted': [1, 0], 'Disappointment/Dissatisfaction': [0, 0, 0], 'Frustrated': [0, 1, 0]}\n",
      "emotion:  sad\n",
      "state:  Satisfaction/Delighted\n",
      "state:  Confusion\n",
      "state:  Disappointment/Dissatisfaction\n",
      "idx 1\n",
      "{'Confusion': [0, 0], 'Satisfaction/Delighted': [1, 0], 'Disappointment/Dissatisfaction': [0, 1, 0], 'Frustrated': [0, 1, 0]}\n",
      "state:  Frustrated\n",
      "idx 0\n",
      "{'Confusion': [0, 0], 'Satisfaction/Delighted': [1, 0], 'Disappointment/Dissatisfaction': [0, 1, 0], 'Frustrated': [1, 1, 0]}\n",
      "emotion:  neutral\n",
      "state:  Satisfaction/Delighted\n",
      "idx 1\n",
      "{'Confusion': [0, 0], 'Satisfaction/Delighted': [1, 1], 'Disappointment/Dissatisfaction': [0, 1, 0], 'Frustrated': [1, 1, 0]}\n",
      "matched state of mind!\n"
     ]
    }
   ],
   "source": [
    "confusion_list = ['neutral', 'supprise'] # duplicated combinatorial emotions for 'sad' with 'Disappointment/dissatisfaction', so 'sad' is removed from 'Confusion'\n",
    "satisfaction_delighterd_list = ['happy', 'neutral']\n",
    "disappointment_disstisfaction_list = ['neutral', 'sad']\n",
    "frustrated_list = ['sad', 'angry', 'neutral']\n",
    "state_of_mind_dict = {'Confusion': confusion_list, \n",
    "'Satisfaction/Delighted': satisfaction_delighterd_list, \n",
    "'Disappointment/Dissatisfaction': disappointment_disstisfaction_list,\n",
    "'Frustrated': frustrated_list}\n",
    "state_of_mind_count = {'Confusion': [0, 0], \n",
    "'Satisfaction/Delighted': [0, 0], \n",
    "'Disappointment/Dissatisfaction': [0, 0, 0],\n",
    "'Frustrated': [0, 0, 0]}\n",
    "state_of_mind_list = ['Confusion', 'Satisfaction/Delighted', 'Disappointment/Dissatisfaction', 'Frustrated']\n",
    "\n",
    "# pass in a dictionary of confidence score for each emotion\n",
    "def get_state_of_mind(emotions_dict):\n",
    "    count = 0\n",
    "    # get the 3 emotions with highest confidence score\n",
    "    sorted_emotions = sorted(emotions_dict, key=emotions_dict.get, reverse=True)[:]\n",
    "    for emotion in sorted_emotions:\n",
    "        print(\"emotion: \", emotion)\n",
    "        # for state in state_of_mind_dict:\n",
    "        for state in state_of_mind_list:\n",
    "            print(\"state: \", state)\n",
    "            # if the emotion belongs to a certain state of mind\n",
    "            if emotion in state_of_mind_dict[state]: \n",
    "                if count == 0:\n",
    "                    state_of_mind_list.remove(state)\n",
    "                    state_of_mind_list.insert(0, state)\n",
    "                    count = 1\n",
    "                # get the index of the emotion in the dictionary\n",
    "                idx = state_of_mind_dict[state].index(emotion)\n",
    "                print(\"idx\", idx)\n",
    "                # set the count to 1\n",
    "                state_of_mind_count[state][idx] = 1\n",
    "                print(state_of_mind_count)\n",
    "            \n",
    "            # check if all emotions of a certain state of mind were matched\n",
    "            if sum(state_of_mind_count[state]) == len(state_of_mind_count[state]):\n",
    "                print(\"matched state of mind!\")\n",
    "                return\n",
    "\n",
    "get_state_of_mind(analyze['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Confusion': [1, 1],\n",
       " 'Satisfaction/Delighted': [1, 1],\n",
       " 'Disappointment/Dissatisfaction': [1, 0, 0],\n",
       " 'Frustrated': [1, 1, 1]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_of_mind_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_of_mind_count['Disappointment/Dissatisfaction'][1] = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_emotions = ['fear', 'happy', 'angry', 'sad', 'neutral', 'surprise', 'disgust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion:  fear\n",
      "state:  Confusion\n",
      "state:  Satisfaction/Delighted\n",
      "state:  Disappointment/Dissatisfaction\n",
      "state:  Frustrated\n",
      "emotion:  happy\n",
      "state:  Confusion\n",
      "state:  Satisfaction/Delighted\n",
      "idx 0\n",
      "{'Confusion': [0, 0], 'Satisfaction/Delighted': [1, 0], 'Disappointment/Dissatisfaction': [0, 999, 0], 'Frustrated': [0, 1, 0]}\n",
      "state:  Disappointment/Dissatisfaction\n",
      "state:  Frustrated\n",
      "emotion:  angry\n",
      "state:  Confusion\n",
      "state:  Satisfaction/Delighted\n",
      "state:  Disappointment/Dissatisfaction\n",
      "state:  Frustrated\n",
      "idx 1\n",
      "{'Confusion': [0, 0], 'Satisfaction/Delighted': [1, 0], 'Disappointment/Dissatisfaction': [0, 999, 0], 'Frustrated': [0, 1, 0]}\n",
      "emotion:  sad\n",
      "state:  Confusion\n",
      "idx 2\n",
      "{'Confusion': [0, 1], 'Satisfaction/Delighted': [1, 0], 'Disappointment/Dissatisfaction': [0, 999, 0], 'Frustrated': [0, 1, 0]}\n",
      "state:  Satisfaction/Delighted\n",
      "state:  Disappointment/Dissatisfaction\n",
      "state:  Frustrated\n",
      "idx 0\n",
      "{'Confusion': [0, 1], 'Satisfaction/Delighted': [1, 0], 'Disappointment/Dissatisfaction': [0, 999, 0], 'Frustrated': [1, 1, 0]}\n",
      "emotion:  neutral\n",
      "state:  Confusion\n",
      "idx 0\n",
      "{'Confusion': [1, 1], 'Satisfaction/Delighted': [1, 0], 'Disappointment/Dissatisfaction': [0, 999, 0], 'Frustrated': [1, 1, 0]}\n",
      "state:  Satisfaction/Delighted\n",
      "idx 1\n",
      "{'Confusion': [1, 1], 'Satisfaction/Delighted': [1, 1], 'Disappointment/Dissatisfaction': [0, 999, 0], 'Frustrated': [1, 1, 0]}\n",
      "state:  Disappointment/Dissatisfaction\n",
      "idx 0\n",
      "{'Confusion': [1, 1], 'Satisfaction/Delighted': [1, 1], 'Disappointment/Dissatisfaction': [1, 999, 0], 'Frustrated': [1, 1, 0]}\n",
      "state:  Frustrated\n",
      "idx 2\n",
      "{'Confusion': [1, 1], 'Satisfaction/Delighted': [1, 1], 'Disappointment/Dissatisfaction': [1, 999, 0], 'Frustrated': [1, 1, 1]}\n",
      "emotion:  surprise\n",
      "state:  Confusion\n",
      "state:  Satisfaction/Delighted\n",
      "state:  Disappointment/Dissatisfaction\n",
      "state:  Frustrated\n",
      "emotion:  disgust\n",
      "state:  Confusion\n",
      "state:  Satisfaction/Delighted\n",
      "state:  Disappointment/Dissatisfaction\n",
      "state:  Frustrated\n"
     ]
    }
   ],
   "source": [
    "# state_of_mind_list = ['Confusion', 'Satisfaction/Delighted', 'Disappointment/Dissatisfaction', 'Frustrated']\n",
    "for emotion in sorted_emotions:\n",
    "    print(\"emotion: \", emotion)\n",
    "    for state in state_of_mind_dict:\n",
    "        print(\"state: \", state)\n",
    "        # if the emotion belongs to a certain state of mind\n",
    "        if emotion in state_of_mind_dict[state]: \n",
    "            # get the index of the emotion in the dictionary\n",
    "            idx = state_of_mind_dict[state].index(emotion)\n",
    "            print(\"idx\", idx)\n",
    "            # special condition where for confusion state of mind the emotion is supprise or sad\n",
    "            if state == 'Confusion' and emotion == 'sad':\n",
    "                idx = 1\n",
    "            # set the count to 1\n",
    "            state_of_mind_count[state][idx] = 1\n",
    "            print(state_of_mind_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral', 'supprise', 'sad']\n",
      "emotion:  fear\n",
      "emotion:  happy\n",
      "emotion:  angry\n",
      "emotion:  sad\n",
      "idx 2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25776\\1041347032.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m                 \u001b[0midx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;31m# set the count to 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mstate_of_mind_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_of_mind_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "state_of_mind_list = ['Confusion', 'Satisfaction/Delighted', 'Disappointment/Dissatisfaction', 'Frustrated']\n",
    "for state in state_of_mind_list:\n",
    "    print(state_of_mind_dict[state])\n",
    "    for emotion in sorted_emotions:\n",
    "        print(\"emotion: \", emotion)\n",
    "        if emotion in state_of_mind_dict[state]:\n",
    "            # find the index of the emotion\n",
    "            idx = state_of_mind_dict[state].index(emotion)\n",
    "            print(\"idx\", idx)\n",
    "            if state == 'Confusion' and emotion == 'sad':\n",
    "                idx = 1\n",
    "            # set the count to 1\n",
    "            state_of_mind_count[state][idx] = 1 \n",
    "            print(state_of_mind_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Confusion': [0, 0],\n",
       " 'Satisfaction/Delighted': [0, 0],\n",
       " 'Disappointment/Dissatisfaction': [0, 0, 0],\n",
       " 'Frustrated': [0, 0, 0]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_of_mind_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# video face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16196\\2133036928.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"video\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m#   cv2_imshow(frame) # for google colab only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;31m# if cv2.waitKey(40) == 27:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;31m# break\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# read the video using VideoCapture funtion\n",
    "video = cv2.VideoCapture(0)\n",
    "# video = cv2.VideoCapture(os.getcwd() + \"/data/clip2_7s.mp4\")\n",
    "# video = cv2.VideoCapture(os.getcwd() + \"/data/vid2_part1.mp4\")\n",
    "# video = cv2.VideoCapture(os.getcwd() + \"/data/vid1_short.mp4\")\n",
    "\n",
    "try:\n",
    "    while video.isOpened():\n",
    "    # read the video into farme\n",
    "        # video.set(cv2.CAP_PROP_FPS,1) \n",
    "        ret, frame = video.read()\n",
    "        # print(video.get(cv2.CAP_PROP_FPS))\n",
    "        # convert the frame to grey for CascadeClassifier\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # find coordinates of faces, returns (x,y,w,h)\n",
    "        # faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor = 1.1, minNeighbors = 5)\n",
    "\n",
    "        for (x,y,w,h) in faces:\n",
    "            frame = cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "            # print(frame)\n",
    "            color = (255,0,0)\n",
    "            try:\n",
    "                analyze = DeepFace.analyze(frame[y:y+h, x:x+w], actions=['emotion'], enforce_detection=False)\n",
    "                # if I set the enforce_detection as False, then though MTCNN is not detecting any face in the picture, the library will consider the whole input image as a face and compute its embedding\n",
    "                # analyze = DeepFace.analyze(image, actions=['emotion'], enforce_detection=False)\n",
    "                # print(analyze)\n",
    "                label = analyze['dominant_emotion']\n",
    "                frame=cv2.putText(frame, label, (x, y - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        # frame[y1:y2, x1:x2]\n",
    "\n",
    "        cv2.imshow(\"video\", frame) \n",
    "        #   cv2_imshow(frame) # for google colab only\n",
    "        cv2.waitKey(5000)\n",
    "        # if cv2.waitKey(40) == 27:\n",
    "            # break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "cv2.destroyAllWindows()\n",
    "video.release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# read the video and extract info about it\n",
    "# cap = cv2.VideoCapture('vid.avi')\n",
    "cap = cv2.VideoCapture(os.getcwd() + \"/data/clip2_7s.mp4\")\n",
    "\n",
    "\n",
    "# get total number of frames and generate a list with each 30 th frame \n",
    "totalFrames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "x = [i for i in range (1, totalFrames) if divmod(i, int(30))[1]==0]\n",
    "\n",
    "\n",
    "for myFrameNumber in x:\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES,myFrameNumber)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        cv2.imshow(\"video\", frame)\n",
    "\n",
    "        ch = 0xFF & cv2.waitKey(1000)\n",
    "        if ch == 27:\n",
    "            break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 60, 90, 120, 150]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.9.2\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: c:\\users\\guang\\anaconda3\\envs\\deepfacepy37\\lib\\site-packages\n",
      "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, keras-preprocessing, libclang, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
      "Required-by: deepface, retina-face\n"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 14 22:51:31 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 451.83       Driver Version: 451.83       CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce MX450      WDDM  | 00000000:5A:00.0 Off |                  N/A |\n",
      "| N/A   47C    P8    N/A /  N/A |    119MiB /  2048MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 10 01:44:05 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 451.83       Driver Version: 451.83       CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce MX450      WDDM  | 00000000:5A:00.0 Off |                  N/A |\n",
      "| N/A   48C    P8    N/A /  N/A |   1432MiB /  2048MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     14196      C   ...s\\deepFacepy37\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('deepFacepy37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21b6fde438783047a9b28b66d2ac1a8262ef604e581950bf28e988f9a8bd21db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
