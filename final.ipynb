{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting GPU-enabled Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      # tf.config.experimental.set_virtual_device_configuration(gpu, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "      # tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "      print(\"sucessful\")\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State of mind Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in a dictionary of confidence score for each emotion\n",
    "def get_state_of_mind(emotions_dict):\n",
    "        \n",
    "    confusion_list = ['neutral', 'surprise'] # duplicated combinatorial emotions for 'sad' with 'Disappointment/dissatisfaction', so 'sad' is removed from 'Confusion'\n",
    "    satisfaction_delighterd_list = ['happy', 'neutral']\n",
    "    disappointment_disstisfaction_list = ['neutral', 'sad']\n",
    "    frustrated_list = ['sad', 'angry', 'neutral']\n",
    "    state_of_mind_dict = {'Confusion': confusion_list, \n",
    "    'Satisfaction/Delighted': satisfaction_delighterd_list, \n",
    "    'Disappointment/Dissatisfaction': disappointment_disstisfaction_list,\n",
    "    'Frustrated': frustrated_list}\n",
    "    state_of_mind_count = {'Confusion': [0, 0], \n",
    "    'Satisfaction/Delighted': [0, 0], \n",
    "    'Disappointment/Dissatisfaction': [0, 0],\n",
    "    'Frustrated': [0, 0, 0]}\n",
    "    state_of_mind_list = ['Confusion', 'Satisfaction/Delighted', 'Disappointment/Dissatisfaction', 'Frustrated']\n",
    "    count = 0\n",
    "    state_switched = False\n",
    "    # sort emotion from highest confidence score to lowest\n",
    "    # sorted_emotions = emotions_dict # TODO: use for testing\n",
    "    sorted_emotions = sorted(emotions_dict, key=emotions_dict.get, reverse=True)[:]\n",
    "    for emotion in sorted_emotions:\n",
    "        # for state in state_of_mind_dict:\n",
    "        for state in state_of_mind_list:\n",
    "            # if the emotion belongs to a certain state of mind\n",
    "            if emotion in state_of_mind_dict[state]: \n",
    "                if count == 0:\n",
    "                    state_of_mind_list.remove(state)\n",
    "                    state_of_mind_list.insert(0, state)\n",
    "                    state_switched = True\n",
    "                # get the index of the emotion in the dictionary\n",
    "                idx = state_of_mind_dict[state].index(emotion)\n",
    "                # set the count to 1\n",
    "                state_of_mind_count[state][idx] = 1\n",
    "            \n",
    "            # check if all emotions of a certain state of mind were matched\n",
    "            if sum(state_of_mind_count[state]) == len(state_of_mind_count[state]):\n",
    "                return state\n",
    "        \n",
    "        if state_switched:\n",
    "            count = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# video face detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# read the video using VideoCapture funtion\n",
    "video = cv2.VideoCapture(0)\n",
    "# video = cv2.VideoCapture(os.getcwd() + \"/data/clip2_7s.mp4\")\n",
    "# video = cv2.VideoCapture(os.getcwd() + \"/data/vid2_part1.mp4\")\n",
    "# video = cv2.VideoCapture(os.getcwd() + \"/data/vid1_short.mp4\")\n",
    "\n",
    "try:\n",
    "    while video.isOpened():\n",
    "    # read the video into farme\n",
    "        # video.set(cv2.CAP_PROP_FPS,1) \n",
    "        ret, frame = video.read()\n",
    "        # print(video.get(cv2.CAP_PROP_FPS))\n",
    "        # convert the frame to grey for CascadeClassifier\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # find coordinates of faces, returns (x,y,w,h)\n",
    "        # faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor = 1.1, minNeighbors = 5)\n",
    "\n",
    "        for (x,y,w,h) in faces:\n",
    "            frame = cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "            # print(frame)\n",
    "            color = (255,0,0)\n",
    "            try:\n",
    "                analyze = DeepFace.analyze(frame[y:y+h, x:x+w], actions=['emotion'], enforce_detection=False)\n",
    "                # if I set the enforce_detection as False, then though MTCNN is not detecting any face in the picture, the library will consider the whole input image as a face and compute its embedding\n",
    "                # analyze = DeepFace.analyze(image, actions=['emotion'], enforce_detection=False)\n",
    "                # print(analyze)\n",
    "                emotion_label = analyze['dominant_emotion']\n",
    "                state_of_mind_label = get_state_of_mind(analyze['emotion'])\n",
    "                frame=cv2.putText(frame, emotion_label, (x, y - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "                frame=cv2.putText(frame, state_of_mind_label, (x, y + h + 12 ),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        # frame[y1:y2, x1:x2]\n",
    "\n",
    "        cv2.imshow(\"video\", frame) \n",
    "        #   cv2_imshow(frame) # for google colab only\n",
    "        # cv2.waitKey(5000)\n",
    "        if cv2.waitKey(40) == 27:\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "cv2.destroyAllWindows()\n",
    "video.release"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "567810847da8aa087eb2f503556f6a98d10762298ab7c386bfd1d2cae854e708"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
