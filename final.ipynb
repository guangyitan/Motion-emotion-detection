{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting GPU-enabled Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      # tf.config.experimental.set_virtual_device_configuration(gpu, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "      # tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "      print(\"sucessful\")\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State of mind Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in a dictionary of confidence score for each emotion\n",
    "def get_state_of_mind(emotions_dict):\n",
    "    # declaring variables    \n",
    "    confusion_list = ['neutral', 'surprise'] # duplicated combinatorial emotions for 'sad' with 'Disappointment/dissatisfaction', so 'sad' is removed from 'Confusion'\n",
    "    satisfaction_delighterd_list = ['happy', 'neutral']\n",
    "    disappointment_disstisfaction_list = ['neutral', 'sad']\n",
    "    frustrated_list = ['sad', 'angry', 'neutral']\n",
    "    # dictionary of ecombinatorial emotions with its respective state of mind\n",
    "    state_of_mind_dict = {'Confusion': confusion_list, \n",
    "    'Satisfaction/Delighted': satisfaction_delighterd_list, \n",
    "    'Disappointment/Dissatisfaction': disappointment_disstisfaction_list,\n",
    "    'Frustrated': frustrated_list}\n",
    "\n",
    "    # dictionary used to store and determin the state of mind based on the combinatorial emotions\n",
    "    state_of_mind_count = {'Confusion': [0, 0], \n",
    "    'Satisfaction/Delighted': [0, 0], \n",
    "    'Disappointment/Dissatisfaction': [0, 0],\n",
    "    'Frustrated': [0, 0, 0]}\n",
    "    state_of_mind_list = ['Confusion', 'Satisfaction/Delighted', 'Disappointment/Dissatisfaction', 'Frustrated']\n",
    "    count = 0\n",
    "    state_switched = False\n",
    "\n",
    "    # sort emotion from highest confidence score to lowest\n",
    "    # sorted_emotions = emotions_dict # TODO: use for testing\n",
    "    sorted_emotions = sorted(emotions_dict, key=emotions_dict.get, reverse=True)[:]\n",
    "    for emotion in sorted_emotions:\n",
    "        for state in state_of_mind_list:\n",
    "            # if the emotion belongs to a certain state of mind\n",
    "            if emotion in state_of_mind_dict[state]: \n",
    "                if count == 0:\n",
    "                    state_of_mind_list.remove(state)\n",
    "                    state_of_mind_list.insert(0, state)\n",
    "                    state_switched = True\n",
    "                # get the index of the emotion in the dictionary\n",
    "                idx = state_of_mind_dict[state].index(emotion)\n",
    "                # set the count to 1\n",
    "                state_of_mind_count[state][idx] = 1\n",
    "            \n",
    "            # check if all emotions of a certain state of mind were matched\n",
    "            if sum(state_of_mind_count[state]) == len(state_of_mind_count[state]):\n",
    "                return state\n",
    "        \n",
    "        if state_switched:\n",
    "            count = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# video face detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "filename = 'video_1'\n",
    "# filename = 'video_2'\n",
    "\n",
    "# declare the facce detection model to detect human faces\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# read the video using VideoCapture funtion\n",
    "# video = cv2.VideoCapture(0)\n",
    "video = cv2.VideoCapture(os.getcwd() + \"/data/\" + filename + \".mp4\")\n",
    "\n",
    "# get width and height\n",
    "frame_width = int(video.get(3))\n",
    "frame_height = int(video.get(4))\n",
    "\n",
    "# declare a VideoWriter instance\n",
    "result_mp4 = cv2.VideoWriter(os.getcwd() + \"/output/\" + filename + '_result.mp4', cv2.VideoWriter_fourcc(*'MP4V'), 10, (frame_width, frame_height))\n",
    "\n",
    "try:\n",
    "    while video.isOpened():\n",
    "        # read the video into frame\n",
    "        ret, frame = video.read()\n",
    "        ret, frame1 = video.read() \n",
    "\n",
    "        # A. face detection, emotion classification, state of mind\n",
    "        # A1. convert the frame to grayscale for CascadeClassifier\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # A2. find coordinates of faces, returns (x,y,w,h)\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor = 1.1, minNeighbors = 5)\n",
    "\n",
    "        # B. motion detection\n",
    "        # B1. find the absolute difference between current frame and previous frame\n",
    "        diff = cv2.absdiff(frame, frame1)\n",
    "        # B2. convert the frame difference to grayscale \n",
    "        gray = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "        # B3. apply gaussian smoothing onto the grayscale frame to reduce noise \n",
    "        blur = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "        # B4. apply thresholding \n",
    "        _, thresh = cv2.threshold(blur, 20, 255, cv2.THRESH_BINARY)\n",
    "        # B5. dilate the frame to remove small unwanted detection\n",
    "        dilated = cv2.dilate(thresh, None, iterations=3)\n",
    "        # B6. get the contours of detected movement in frame\n",
    "        contours, _ = cv2.findContours(dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # legends in video\n",
    "        cv2.putText(frame, \"Blue: Emotion\", (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "        cv2.putText(frame, \"Red: State of Mind\", (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "        cv2.putText(frame, \"Green: Motion\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # loop through all detected faces\n",
    "        for (x,y,w,h) in faces:\n",
    "            color = (255,0,0) # blue color\n",
    "            # A3. draw a blue rectangle around the face\n",
    "            frame = cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            try:\n",
    "                # A4. perform emotion classification on the face detected\n",
    "                analyze = DeepFace.analyze(frame[y:y+h, x:x+w], actions=['emotion'], enforce_detection=False)\n",
    "                # A5. get the dominant emotion as the label \n",
    "                emotion_label = analyze['dominant_emotion']\n",
    "                # A6. perform state of mind calculation\n",
    "                state_of_mind_label = get_state_of_mind(analyze['emotion'])\n",
    "                # A7. write the emotion on the frame in blue text\n",
    "                frame=cv2.putText(frame, emotion_label, (x, y - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "                # A8. write the state of mind on the frame in red text\n",
    "                frame=cv2.putText(frame, state_of_mind_label, (x, y + h + 12 ),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        # loop through all contours\n",
    "        for contour in contours:\n",
    "            # B7. generate x, y coordinate, width, height based on the contour\n",
    "            (x, y, w, h) = cv2.boundingRect(contour)\n",
    "            # B8. draw a green rectangle if the contour area is greater than 600\n",
    "            if cv2.contourArea(contour) > 600:\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        result_mp4.write(frame)\n",
    "\n",
    "        cv2.imshow(\"video\", frame) \n",
    "        frame = frame1\n",
    "        ret, frame1 = video.read()\n",
    "        #   cv2_imshow(frame) # for google colab only\n",
    "        # cv2.waitKey(5000)\n",
    "        if cv2.waitKey(40) == 27:\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release\n",
    "    result_mp4.release()\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "cv2.destroyAllWindows()\n",
    "video.release\n",
    "result_mp4.release()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('deepFacepy37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21b6fde438783047a9b28b66d2ac1a8262ef604e581950bf28e988f9a8bd21db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
